{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# 初期設定",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%session_id_prefix native-iceberg-dataframe-\n%glue_version 5.0\n%idle_timeout 60\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nSetting session ID prefix to native-iceberg-dataframe-\nSetting Glue version to: 5.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit\n\n\n# Sparkセッションの設定\nspark = SparkSession.builder \\\n    .appName(\"GlueNotebook-Iceberg\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.glue_catalog.warehouse\", \"s3://waptest20241208/warehouse/\") \\\n    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .getOrCreate()\n\n# データの読み込み\nrawDataPath = \"s3://waptest20241208/raw_data/\"\nauditDf = spark.read.format(\"parquet\").load(f\"{rawDataPath}/nyc_tlc/yellow/mar2024/\")\n\n# Iceberg のブランチ設定\nprodTable = \"glue_catalog.nyc.yellow_taxi_trips\"\nauditBranch = \"dataAudit_202403\"\n\n# WAP 処理を有効化\nspark.sql(f\"ALTER TABLE {prodTable} SET TBLPROPERTIES ('write.wap.enabled'='true')\")\nspark.sql(f\"ALTER TABLE {prodTable} CREATE BRANCH {auditBranch}\")\nspark.conf.set(\"spark.wap.branch\", auditBranch)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nIdle Timeout: 60\nSession ID: 59ce8257-b22a-439a-92c5-79dda5f66a79\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\nWaiting for session 59ce8257-b22a-439a-92c5-79dda5f66a79 to get into ready status...\nSession 59ce8257-b22a-439a-92c5-79dda5f66a79 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Write",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# データに年と月を追加\nauditDf = auditDf.withColumn(\"month\", lit(3)).withColumn(\"year\", lit(2024))\n\n# データを書き込み\nauditDf.writeTo(prodTable).append()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# 中間データの確認",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Auditブランチ\nspark.table(prodTable).groupBy(\"year\", \"month\").count().show()\n\n# mainブランチ\nspark.read.option(\"BRANCH\", \"main\").table(prodTable).groupBy(\"year\", \"month\").count().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2024|    2|3007526|\n|2024|    3|3582628|\n+----+-----+-------+\n\n+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2024|    2|3007526|\n+----+-----+-------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Audit/Publish",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n# Auditブランチからデータのロード\nauditData = spark.read.option(\"BRANCH\", auditBranch).table(prodTable)\n\n# Audit\nmonthDf = auditData.filter(col(\"month\") == 2)\n\nif monthDf.isEmpty():\n    # データの登録をスキップ\n    # SNSで通知するのがいいかなと\n    print(\"No data found for month = 2. Skipping registration and sending notification.\")\nelse:\n    # データの登録\n    spark.sql(f\"\"\"CALL glue_catalog.system.fast_forward('{prodTable}', 'main', '{auditBranch}')\"\"\")\n    print(\"Data registered successfully.\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[branch_updated: string, previous_ref: bigint, updated_ref: bigint]\nData registered successfully.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# データの登録を確認",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Auditブランチ\nspark.table(prodTable).groupBy(\"year\", \"month\").count().show()\n\n# mainブランチ\nspark.read.option(\"BRANCH\", \"main\").table(prodTable).groupBy(\"year\", \"month\").count().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2024|    2|3007526|\n|2024|    3|3582628|\n+----+-----+-------+\n\n+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2024|    2|3007526|\n|2024|    3|3582628|\n+----+-----+-------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# 後処理",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(f\"ALTER TABLE {prodTable} UNSET TBLPROPERTIES ('write.wap.enabled')\")\n\nspark.sql(f\"ALTER TABLE {prodTable} DROP BRANCH {auditBranch}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 59ce8257-b22a-439a-92c5-79dda5f66a79\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}